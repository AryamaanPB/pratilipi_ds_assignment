{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71048a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1723f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_datetime(data, column_name=\"updated_at\"):\n",
    "    \"\"\"\n",
    "    Converts column to datetime format\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        column_name (str) : column which needs to be converted to datetime\n",
    "        \n",
    "    Returns:\n",
    "            data (pandas.dataframe) : updated data with datetime column.\n",
    "    \"\"\"\n",
    "    data[column_name] = pd.to_datetime(data[column_name])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a223513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sort_values(data, column_name=\"updated_at\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Sort values according to a specific column\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        column_name (str) : column on which the dataframe needs to be sorted.\n",
    "        \n",
    "    Returns:\n",
    "            data (pandas.dataframe) : updated data with sorted rows.\n",
    "    \"\"\"\n",
    "    data = data.sort_values(column_name).reset_index(drop=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435dc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _user_cleanup(data, user_column_name=\"user_id\", threshold=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Clean-up the data by dropping all users that have not read books more than the threshold\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        user_column_name (str) : column which signifies the user_id.\n",
    "        threshold (int) : number of books the user should have read.\n",
    "        \n",
    "    Returns:\n",
    "            data (pandas.dataframe) : updated data without the unnecessary rows.\n",
    "    \"\"\"\n",
    "\n",
    "    x = data[user_column_name].value_counts() > threshold\n",
    "    y = x[x].index  # user_ids\n",
    "    data = data[data[\"user_id\"].isin(y)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39184483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _final_merge(\n",
    "    user_data,\n",
    "    metadata,\n",
    "    metadata_columns=[\"pratilipi_id\", \"category_name\"],\n",
    "    merge_column=\"pratilipi_id\",\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Merging the userdata and metadata on a specified merge_column.\n",
    "    \n",
    "    Args:\n",
    "        user_data (pandas.dataframe) : user interaction data that has been cleaned up, sorted.\n",
    "        metadata (pandas.dataframe) : metadata without the column 'updated_at'\n",
    "        metadata_columns (list) : the columns you want to use from the metadata table during the final merge.\n",
    "        merge_column (str) : column on which you want to merge the two dataframes.\n",
    "        \n",
    "    Returns:\n",
    "            complete_data (pandas.dataframe) : final compiled data after merging user_data and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    complete_data = user_data.merge(metadata[metadata_columns], on=merge_column)\n",
    "\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_duplicates(data, columns=[\"user_id\", \"pratilipi_id\"], keep=\"last\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to help drop columns or a list of columns from a dataframe\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        columns (list) : all columns we want to drop from the dataframe\n",
    "        keep (str) : specifies whether you want to keep the first of last instance of the duplicate value.\n",
    "        \n",
    "    Returns:\n",
    "            data (pandas.dataframe) : updated data with the columns we want. \n",
    "    \"\"\"\n",
    "    data = data.drop_duplicates(columns, keep=keep).reset_index(drop=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_encode(data, value=\"category_id\", encode_column=\"category_name\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to label encode the categorical columns we want to use as values during the pivot.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        value (str) : final name of the encoded column.\n",
    "        encode_column (str) : name of the column we want to encode.\n",
    "    \n",
    "    Returns:\n",
    "            data (pandas.dataframe) : updated dataframe with encoded columns.\n",
    "    \"\"\"\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    data[value] = le.fit_transform(data[encode_column])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16769850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prune_data(\n",
    "    complete_data,\n",
    "    index=\"pratilipi_id\",\n",
    "    column=\"user_id\",\n",
    "    value=\"category_id\",\n",
    "    threshold=10,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to prune the complete data where only categories which entries more than the threshold are to be kept.\n",
    "    \n",
    "    Args:\n",
    "        complete_data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        index (str) : index for the pivot table.\n",
    "        column (str) : columns we want for the pivot table.\n",
    "        value (str) : values to be used in the pivot table.\n",
    "        threshold (int) : minimum number of categories.\n",
    "        \n",
    "    Returns:\n",
    "            final_val (pandas.dataframe) : Final dataframe with all the values we want.\n",
    "    \"\"\"\n",
    "\n",
    "    count_val = complete_data.groupby(index)[value].count().reset_index()\n",
    "\n",
    "    count_val.rename(columns={value: \"count_{}\".format(value)}, inplace=True)\n",
    "\n",
    "    final_val = complete_data.merge(count_val, on=index)\n",
    "\n",
    "    final_val = final_val[final_val[\"count_{}\".format(value)] >= threshold]\n",
    "    final_val.drop_duplicates([column, index], inplace=True)\n",
    "\n",
    "    return final_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pivot(\n",
    "    final_val,\n",
    "    threshold=216521472,\n",
    "    index=\"pratilipi_id\",\n",
    "    column=\"user_id\",\n",
    "    value=\"category_id\",\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to pivot the final value table.\n",
    "    \n",
    "    Args:\n",
    "        final_val (pandas.dataframe) : Final dataframe with all the values we want.\n",
    "        threshold (int) : due to memory limitation I was forced to use this constraint.\n",
    "        index (str) : index for the pivot table.\n",
    "        column (str) : columns we want for the pivot table.\n",
    "        value (str) : values to be used in the pivot table.\n",
    "        \n",
    "    Returns:\n",
    "            table_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    table_pivot = final_val.iloc[:threshold].pivot_table(\n",
    "        columns=column, index=index, values=value\n",
    "    )\n",
    "\n",
    "    table_pivot.fillna(0, inplace=True)\n",
    "\n",
    "    return table_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a42a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_most_popular(complete_data):\n",
    "\n",
    "    \"\"\"\n",
    "    A function to get a list of the most popular pratilipis from each category. Used incase we aren't able to find a nearest neighbor.\n",
    "    \n",
    "    Args:\n",
    "        complete_data (pandas.dataframe) : data on which said action needs to be performed.\n",
    "        \n",
    "    Returns:\n",
    "            popular_pratilipis (dict) : a dictionary with a list of pratilipis descending according to their popularity.\n",
    "    \"\"\"\n",
    "\n",
    "    categs = complete_data[\"category_name\"].unique().tolist()\n",
    "\n",
    "    popular_pratilipis = {}\n",
    "\n",
    "    for category_name in categs:\n",
    "\n",
    "        data = complete_data[complete_data[\"category_name\"] == category_name]\n",
    "\n",
    "        pratilipi_counts = (\n",
    "            data.groupby(\"pratilipi_id\")[\"category_name\"].count().reset_index()\n",
    "        )\n",
    "        pratilipi_counts.rename(\n",
    "            columns={\"category_name\": \"count_pratilipi\"}, inplace=True\n",
    "        )\n",
    "\n",
    "        data = data.merge(pratilipi_counts, on=\"pratilipi_id\")\n",
    "        data = data.sort_values(\"count_pratilipi\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        list_pratilipi = data[\"pratilipi_id\"].unique().tolist()\n",
    "\n",
    "        popular_pratilipis.update({category_name: list_pratilipi})\n",
    "\n",
    "    return popular_pratilipis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sparse(table_pivot):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to get a sparse matrix of the pivot table to be able to fit in the KNN algorithm.\n",
    "    \n",
    "    Args:\n",
    "        table_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "        \n",
    "    Returns:\n",
    "            table_sparse (csr matrix) : a sparse matrix of the pivot table.\n",
    "    \"\"\"\n",
    "\n",
    "    table_sparse = csr_matrix(table_pivot)\n",
    "\n",
    "    return table_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to read a csv from the file specified and return a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str) : path to the csv file.\n",
    "    \n",
    "    Returns:\n",
    "            sample (pandas.dataframe) : csv loaded as a dataframe.\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(filepath)\n",
    "\n",
    "    if set([\"Unnamed: 0\"]).issubset(sample.columns):\n",
    "        sample.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf063c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    user_data,\n",
    "    metadata,\n",
    "    datetime_column=\"updated_at\",\n",
    "    user_threshold=10,\n",
    "    user_column_name=\"user_id\",\n",
    "    test_size=0.25,\n",
    "    merge_column=\"pratilipi_id\",\n",
    "    duplicate_columns=[\"user_id\", \"pratilipi_id\"],\n",
    "    metadata_columns=[\"pratilipi_id\", \"category_name\"],\n",
    "    keep=\"last\",\n",
    "    label_column=\"category_name\",\n",
    "    index=\"pratilipi_id\",\n",
    "    column=\"user_id\",\n",
    "    value=\"category_id\",\n",
    "    category_threshold=10,\n",
    "    table_threshold=1000000,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to process all the data and return all necessary values.\n",
    "    \n",
    "    Args:\n",
    "        user_data (pandas.dataframe) : user interaction data that has been cleaned up, sorted.\n",
    "        metadata (pandas.dataframe) : metadata without the column 'updated_at'\n",
    "        datetime_column (str) : name of the column that needs to be converted to datetime.\n",
    "        user_threshold (int) : minimum number of books the user must have read.\n",
    "        user_column_name (str) : name of the column that holds the user id.\n",
    "        test_size (float) : percentage of the total data we want to use as test data.\n",
    "        merge_column (str) : column on which you want to merge the two dataframes.\n",
    "        duplicate_columns (list) : list of all columns using which we want to drop the duplicated rows.\n",
    "        metadata_columns (list) : the columns you want to use from the metadata table during the final merge.\n",
    "        keep (str) : specifies whether you want to keep the first of last instance of the duplicate value.\n",
    "        label_column (str) : name of the column we want to encode.\n",
    "        index (str) : index for the pivot table.\n",
    "        column (str) : columns we want for the pivot table.\n",
    "        value (str) : values to be used in the pivot table.\n",
    "        category_threshold (int) : minimum number of categories that need to be present.\n",
    "        table_threshold (int) : memory constraint.\n",
    "        \n",
    "    Returns:\n",
    "            table_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "            table_sparse (csr matrix) : a sparse matrix of the pivot table.\n",
    "            test_data (pandas.dataframe) : split of the combined data to be used for testing.\n",
    "            complete_data (pandas.dataframe) : compiled data.\n",
    "            metadata (pandas.dataframe) : updated metadata.\n",
    "            user_data (pandas.dataframe) : updated user_data.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"converting columns to datetime\")\n",
    "    user_data = _to_datetime(user_data, datetime_column)\n",
    "    #     metadata = _to_datetime(metadata, datetime_column)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"sorting dataframe according to date\")\n",
    "    user_data = _sort_values(user_data, datetime_column)\n",
    "    #     metadata = _sort_values(metadata, datetime_column)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"cleaning up user data\")\n",
    "    user_data = _user_cleanup(user_data, user_column_name, user_threshold)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"dropping duplicate pratilipi ids from metadata\")\n",
    "    metadata = _drop_duplicates(metadata, [\"pratilipi_id\"], keep)\n",
    "\n",
    "    print(\"dropping updated_at column from metadata\")\n",
    "    metadata.drop(\"updated_at\", axis=1, inplace=True)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"merging user data and metadata on pratilipi_id\")\n",
    "    complete_data = _final_merge(user_data, metadata, metadata_columns, merge_column)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"dropping duplicates on complete data\")\n",
    "    complete_data = _drop_duplicates(complete_data, duplicate_columns, keep)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"sorting complete data according to date\")\n",
    "    complete_data = _sort_values(complete_data, datetime_column)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"splitting train and test data\")\n",
    "    train_data, test_data = train_test_split(\n",
    "        complete_data, test_size=test_size, shuffle=False\n",
    "    )\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    display(train_data.head())\n",
    "\n",
    "    print(\"encoding labels\")\n",
    "    train_data = _label_encode(train_data, value, label_column)\n",
    "    #     train_data = _label_encode(train_data, 'user_id', 'user_id')\n",
    "    #     train_data = _label_encode(train_data, 'pratilipi_id', 'pratilipi_id')\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    display(train_data.head())\n",
    "\n",
    "    print(\"pruning data\")\n",
    "    final_val = _prune_data(train_data, index, column, value, category_threshold)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    display(final_val.head())\n",
    "\n",
    "    print(\"getting pivot table\")\n",
    "    table_pivot = _get_pivot(final_val, table_threshold, index, column, value)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"get sparse table\")\n",
    "    table_sparse = _get_sparse(table_pivot)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    display(test_data.head())\n",
    "\n",
    "    return table_pivot, table_sparse, test_data, complete_data, metadata, user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(table_sparse, n_neighbors=10, algorithm=\"brute\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function used to fit the sparse matrix into a model to be used later to predict K nearest neighbors.\n",
    "    \n",
    "    Args:\n",
    "        table_sparse (csr matrix) : a sparse matrix of the pivot table.\n",
    "        n_neighbors (int) : number of neighbors we want to predict.\n",
    "        algorithm (str) : the algorithm we want to use to build our model.\n",
    "        \n",
    "    Returns:\n",
    "            model (sklearn.neighbors._unsupervised.NearestNeighbors) : model that can be used to predict K nearest books.\n",
    "    \"\"\"\n",
    "    model = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm)\n",
    "    model.fit(table_sparse)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d049cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_user_prediction_stats(\n",
    "    user_id, popular_pratilipis, test_data, train_pivot, metadata, model\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to get prediction performance statitics on a per-user basis.\n",
    "    \n",
    "    Args:\n",
    "        user_id (int) : user id.\n",
    "        popular_pratilipis (dict) : a dictionary with a list of pratilipis descending according to their popularity.\n",
    "        test_data (pandas.dataframe) : split of the combined data to be used for testing.\n",
    "        train_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "        metadata (pandas.dataframe) : updated metadata containing information about pratilipis.\n",
    "        model (sklearn.neighbors._unsupervised.NearestNeighbors) : model that can be used to predict K nearest books.\n",
    "        \n",
    "    Returns:\n",
    "            c_p (float) : percentage of cases where the category of the prediction matched that of the actual pratilipis.\n",
    "            p_p (float) : percentage of cases where the pratilipi_id of the prediction matched that of the actual pratilipis.\n",
    "            final_pred (list) : list of top 5 pratilipi predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    c_p = 0.0\n",
    "    p_p = 0.0\n",
    "    final_pred = []\n",
    "    books_pred, user_category, user_pratilipi = _get_pred_dict(\n",
    "        user_id, popular_pratilipis, test_data, train_pivot, model\n",
    "    )\n",
    "    if len(books_pred) != 0:\n",
    "        final_pred = _get_predictions(books_pred)\n",
    "        c_p, p_p = _evaluate_predictions(\n",
    "            final_pred, metadata, user_category, user_pratilipi\n",
    "        )\n",
    "\n",
    "    return c_p, p_p, final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(model, table_pivot, book_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to predict K nearest pratilipis from the specified book_id.\n",
    "    \n",
    "    Args:\n",
    "        model (sklearn.neighbors._unsupervised.NearestNeighbors) : model that can be used to predict K nearest books.\n",
    "        table_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "        book_id (int) : pratilipi id of the book you want to use to predict its neighbors.\n",
    "        \n",
    "    Returns:\n",
    "            distances (list) : list of distances of the prediction from the actual value.\n",
    "            suggestions (list) : list of all suggestion.\n",
    "    \"\"\"\n",
    "\n",
    "    distances, suggestions = model.kneighbors(\n",
    "        table_pivot.loc[table_pivot.index == book_id].values.reshape(1, -1)\n",
    "    )\n",
    "\n",
    "    return distances, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97608fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pred_dict(user_id, popular_pratilipis, test_data, train_pivot, model):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to get a dictionary of all predicted pratilipis and their distances from the actual pratilipi.\n",
    "    \n",
    "    Args:\n",
    "        user_id (int) : user id.\n",
    "        popular_pratilipis (dict) : a dictionary with a list of pratilipis descending according to their popularity.\n",
    "        test_data (pandas.dataframe) : split of the combined data to be used for testing.\n",
    "        train_pivot (pandas.dataframe) : pivot of the table for us to find K nearest neighbors.\n",
    "        model (sklearn.neighbors._unsupervised.NearestNeighbors) : model that can be used to predict K nearest books.\n",
    "\n",
    "    Returns:\n",
    "            books_pred (dict) : a dictionary of all predicted pratilipis and their distances from the actual pratilipi.\n",
    "            user_category (list) : list of all the categories that the user has read.\n",
    "            user_pratilipi (list) : list of all the pratilipis the user has read.\n",
    "    \"\"\"\n",
    "\n",
    "    user = test_data.loc[test_data[\"user_id\"] == user_id]\n",
    "    user = user.sort_values(\"updated_at\", ascending=False).reset_index(drop=True)\n",
    "    user_pratilipi = user[\"pratilipi_id\"].tolist()\n",
    "    user_category = user[\"category_name\"].tolist()\n",
    "    used_category = {}\n",
    "    unique_ids = train_pivot.index.unique().tolist()\n",
    "\n",
    "    books_pred = {}\n",
    "\n",
    "    i = 0\n",
    "    for idd in user_pratilipi:\n",
    "        if idd in unique_ids:\n",
    "            distances, suggestions = _predict(model, train_pivot, idd)\n",
    "\n",
    "            suggested_id = train_pivot.index[suggestions[0][1]]\n",
    "            distance = distances[0][1]\n",
    "        else:\n",
    "            if user_category[i] in used_category:\n",
    "                n = used_category[user_category[i]] + 1\n",
    "            else:\n",
    "                n = 0\n",
    "\n",
    "            used_category.update({user_category[i]: n})\n",
    "\n",
    "            suggested_id = popular_pratilipis.get(user_category[i])[n]\n",
    "            distance = 100.0\n",
    "\n",
    "        books_pred.update({suggested_id: distance})\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return books_pred, user_category, user_pratilipi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fa49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_predictions(books_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to get the top K predictions from the dictionary.\n",
    "    \n",
    "    Args:\n",
    "        books_pred (dict) : a dictionary of all predicted pratilipis and their distances from the actual pratilipi.\n",
    "    \n",
    "    Returns:\n",
    "            final_pred (list) : list of top K predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    final_pred = []\n",
    "    i = 0\n",
    "    for w in sorted(books_pred, key=books_pred.get, reverse=False):\n",
    "        if i < 5:\n",
    "            final_pred.append(w)\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_predictions(final_pred, metadata, user_category, user_pratilipi):\n",
    "\n",
    "    \"\"\"\n",
    "    A function to evaluate the predictions made by the model.\n",
    "    \n",
    "    Args:\n",
    "        final_pred (list) : list of top K predictions.\n",
    "        metadata (pandas.dataframe) : updated metadata containing information about pratilipis.\n",
    "        user_category (list) : list of all the categories that the user has read.\n",
    "        user_pratilipi (list) : list of all the pratilipis the user has read.\n",
    "        \n",
    "    Returns:\n",
    "            category_match_percentage (list) : list of all category matches percentage.\n",
    "            pratilipi_match_percentage (float) : list of all pratilipi matches percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(final_pred)\n",
    "    category_match = 0\n",
    "    pratilipi_match = 0\n",
    "    category_match_percentage = 0.0\n",
    "    pratilipi_match_percentage = 0.0\n",
    "    if total != 0:\n",
    "        for pred in final_pred:\n",
    "            category = metadata.loc[metadata[\"pratilipi_id\"] == pred][\n",
    "                \"category_name\"\n",
    "            ].tolist()[0]\n",
    "            if category in user_category:\n",
    "                category_match += 1\n",
    "            if pred in user_pratilipi:\n",
    "                pratilipi_match += 1\n",
    "\n",
    "        category_match_percentage = 100 * (category_match / total)\n",
    "        pratilipi_match_percentage = 100 * (pratilipi_match / total)\n",
    "\n",
    "    return category_match_percentage, pratilipi_match_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_to_test_data(test_data, categ_perc, pratilipi_perc):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to add the evaluation metrics as a column to the test data to be able to evaluate on a per user basis.\n",
    "    \n",
    "    Args:\n",
    "        test_data (pandas.dataframe) : split of the combined data to be used for testing.\n",
    "        categ_perc (list) : list of all category matches percentage.\n",
    "        pratilipi_perc (float) : list of all pratilipi matches percentage.\n",
    "        \n",
    "    Returns:\n",
    "            test_data (pandas.dataframe) : updated test data with all evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    test_data[\"category_match_percentage\"] = categ_perc\n",
    "    test_data[\"pratilipi_match_percentage\"] = pratilipi_perc\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(test_data, complete_data, metadata, train_pivot, model):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to predict and evaluate on the entire test dataset.\n",
    "    \n",
    "    Args:\n",
    "        test_data (pandas.dataframe) : data that was split off to be used for testing.\n",
    "        complete_data (pandas.dataframe) : processed and compiled data.\n",
    "        metadata (pandas.dataframe) : updated metadata containing information about the pratilipis.\n",
    "        train_pivot (pandas.dataframe) : pivotted table.\n",
    "        model (sklearn.neighbors._unsupervised.NearestNeighbors) : model that can be used to predict K nearest books.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "            test_data (pandas.dataframe) : updated and final test data with all evaluation metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    user_ids = test_data[\"user_id\"].tolist()\n",
    "\n",
    "    popular_pratilipis = _get_most_popular(complete_data)\n",
    "\n",
    "    categ_match_perc = []\n",
    "    pratilipi_match_perc = []\n",
    "\n",
    "    for u_id in user_ids:\n",
    "        cp, pp, fin_pred = _get_user_prediction_stats(\n",
    "            u_id, popular_pratilipis, test_data, train_pivot, metadata, model\n",
    "        )\n",
    "        categ_match_perc.append(cp)\n",
    "        pratilipi_match_perc.append(pp)\n",
    "\n",
    "    test_data = _add_to_test_data(test_data, categ_match_perc, pratilipi_match_perc)\n",
    "\n",
    "    return test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
